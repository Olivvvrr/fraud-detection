{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for memory analysis and DataFrame information\n",
    "def analyze_memory(df):\n",
    "    print(\"\\n=== Memory Analysis and DataFrame Information ===\")\n",
    "    memory = df.memory_usage(deep(True).sum() / 1024 ** 2\n",
    "    print(f\"Total memory used by DataFrame: {memory:.2f} MB\")\n",
    "    print(\"\\nDetailed DataFrame information:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "df = pd.read_csv(r\"creditcard.csv\")\n",
    "# Perform memory analysis and DataFrame information\n",
    "analyze_memory(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"\\nStatistical description:\")\n",
    "print(df.describe().T)\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Distribution of the target variable (Class)\n",
    "sns.countplot(data=df, x='Class', ax=axes[0])\n",
    "axes[0].set_title('Class Distribution')\n",
    "# Analysis of the distribution of the Amount variable\n",
    "sns.histplot(df['Amount'], bins=50, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Amount Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-variables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating independent and dependent variables\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "# Analysis of class imbalance\n",
    "print(\"\\nClass Distribution (Imbalanced):\")\n",
    "print(y.value_counts(normalize(True))\n",
    "# Normalization of the Amount variable\n",
    "scaler = StandardScaler()\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of independent and dependent variables\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply-smote",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE only to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    start = time.time()\n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    mean_scores = scores.mean()\n",
    "    # Training\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predictions and evaluation on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    # Calculation of AUC-ROC\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    execution_time = time.time() - start\n",
    "    return {\n",
    "        \"scores\": scores,\n",
    "        \"mean_scores\": mean_scores,\n",
    "        \"report\": report,\n",
    "        \"confusion_matrix\": confusion_matrix,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"execution_time\": execution_time\n",
    "    }\n",
    "# Models to be evaluated\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "    \"Neural Network\": MLPClassifier(random_state=42, max_iter=300, early_stopping=True, validation_fraction=0.1, n_iter_no_change=15, verbose=True),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "}\n",
    "# Model evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating model: {name}\")\n",
    "    results[name] = evaluate_model(model, X_train_res, y_train_res, X_test, y_test)\n",
    "# Displaying the results\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Cross-Validation Scores:\", result['scores'])\n",
    "    print(\"Mean Scores:\", result['mean_scores'])\n",
    "    print(\"Classification Report:\\n\", result['report'])\n",
    "    print(\"Confusion Matrix:\\n\", result['confusion_matrix'])\n",
    "    print(\"AUC-ROC:\\n\", result['roc_auc'])\n",
    "    print(\"Execution Time (s):\\n\", result['execution_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Random Forest model as an example\n",
    "import joblib\n",
    "joblib.dump(models[\"Random Forest\"], 'fraud_detection_model_rf.pkl')\n",
    "print(\"\\nRandom Forest model saved as 'fraud_detection_model_rf.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
